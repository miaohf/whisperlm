# WhisperLM

ä¸€ä¸ªç»“åˆ WhisperX ç²¾ç¡®è½¬å½•å’Œ LLM è¯­ä¹‰ä¼˜åŒ–çš„æ™ºèƒ½è¯­éŸ³è½¬æ–‡å­—æœåŠ¡ã€‚

## âœ¨ ç‰¹æ€§

- ğŸ¯ **ç²¾ç¡®è½¬å½•**ï¼šåŸºäº WhisperX çš„è¯çº§æ—¶é—´æˆ³å¯¹é½ï¼ˆç²¾åº¦ ~50msï¼‰
- ğŸ‘¥ **è¯´è¯äººåˆ†ç¦»**ï¼šè‡ªåŠ¨è¯†åˆ«å’Œæ ‡æ³¨ä¸åŒè¯´è¯äººï¼ˆåŸºäº pyannoteï¼‰
- ğŸ§  **è¯­ä¹‰ä¼˜åŒ–**ï¼šLLM æ™ºèƒ½æ–­å¥ã€ä¿®å¤ ASR é”™è¯¯ã€ä¼˜åŒ–è¡¨è¾¾
- ğŸŒ **å¤šè¯­è¨€ç¿»è¯‘**ï¼šæ”¯æŒ 100+ è¯­è¨€çš„é«˜è´¨é‡ç¿»è¯‘
- ğŸš€ **é«˜æ€§èƒ½**ï¼šæ”¯æŒ GPU åŠ é€Ÿï¼Œæ‰¹é‡å¤„ç†
- ğŸ“¡ **RESTful API**ï¼šæ˜“äºé›†æˆçš„ HTTP æ¥å£
- ğŸ”„ **ä¸€ä½“åŒ–å¤„ç†**ï¼šè½¬å½•ã€å¯¹é½ã€è¯´è¯äººåˆ†ç¦»ä¸€æ¬¡å®Œæˆ

## ğŸ—ï¸ æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         WhisperLM                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  éŸ³é¢‘    â”‚â”€â”€â”€â–ºâ”‚ WhisperX â”‚â”€â”€â”€â–ºâ”‚   LLM    â”‚â”€â”€â”€â–ºâ”‚  è¾“å‡º    â”‚  â”‚
â”‚  â”‚  è¾“å…¥    â”‚    â”‚  Pipelineâ”‚    â”‚  ä¼˜åŒ–å™¨  â”‚    â”‚  å­—å¹•    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                       â”‚                â”‚                        â”‚
â”‚                       â–¼                â–¼                        â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚                 â”‚ Whisper  â”‚    â”‚ è¯­ä¹‰åˆ†æ®µ â”‚                   â”‚
â”‚                 â”‚ wav2vec2 â”‚    â”‚ é”™è¯¯ä¿®å¤ â”‚                   â”‚
â”‚                 â”‚ pyannote â”‚    â”‚ ç¿»è¯‘ä¼˜åŒ– â”‚                   â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ å¤„ç†æµç¨‹

```
1. éŸ³é¢‘è¾“å…¥
      â”‚
      â–¼
2. VAD é¢„å¤„ç†ï¼ˆè¿‡æ»¤é™éŸ³æ®µï¼‰
      â”‚
      â–¼
3. Whisper è½¬å½•ï¼ˆç”Ÿæˆåˆå§‹æ–‡æœ¬ï¼‰
      â”‚
      â–¼
4. wav2vec2 è¯çº§å¯¹é½ï¼ˆç²¾ç¡®æ—¶é—´æˆ³ï¼‰
      â”‚
      â–¼
5. pyannote è¯´è¯äººåˆ†ç¦»ï¼ˆæ ‡æ³¨è¯´è¯äººï¼‰
      â”‚
      â–¼
6. LLM è¯­ä¹‰å¤„ç†ï¼ˆå¯é€‰ï¼‰
   â”œâ”€â”€ æ™ºèƒ½æ–­å¥ï¼ˆæŒ‰è¯­ä¹‰è¾¹ç•Œï¼‰
   â”œâ”€â”€ ASR é”™è¯¯ä¿®å¤
   â”œâ”€â”€ è¡¨è¾¾ä¼˜åŒ–
   â””â”€â”€ å¤šè¯­è¨€ç¿»è¯‘
      â”‚
      â–¼
7. è¾“å‡ºå­—å¹•ï¼ˆJSON/SRT/VTTï¼‰
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python >= 3.10
- CUDA >= 11.8ï¼ˆæ¨èï¼Œæ”¯æŒ CPU ä½†è¾ƒæ…¢ï¼‰
- FFmpeg >= 4.0
- 8GB+ GPU æ˜¾å­˜ï¼ˆæ¨è 16GB+ï¼‰

### å®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/whisperlm.git
cd whisperlm

# ä½¿ç”¨ uv å®‰è£…ä¾èµ–ï¼ˆæ¨èï¼‰
uv sync

# æˆ–ä½¿ç”¨ pip
pip install -e .
```

### é…ç½®

å¤åˆ¶é…ç½®æ–‡ä»¶å¹¶ä¿®æ”¹ï¼š

```bash
cp config.example.yaml config.yaml
```

ç¼–è¾‘ `config.yaml`ï¼š

```yaml
# æœåŠ¡é…ç½®
server:
  host: "0.0.0.0"
  port: 8001
  workers: 1

# WhisperX é…ç½®ï¼ˆæ¨èï¼šfloat16ï¼‰
whisperx:
  model: "large-v3"           # tiny, base, small, medium, large-v2, large-v3
  device: "cuda"              # cuda, cpu
  compute_type: "float16"     # float16 ç²¾åº¦æ›´é«˜ï¼Œæ˜¾å­˜å ç”¨ ~8GB
  batch_size: 16
  language: null              # è‡ªåŠ¨æ£€æµ‹ï¼Œæˆ–æŒ‡å®šå¦‚ "en", "zh"

# è¯´è¯äººåˆ†ç¦»é…ç½®
diarization:
  enabled: true
  huggingface_token: "${HF_TOKEN}"  # éœ€è¦ç”³è¯· pyannote æƒé™
  min_speakers: null          # æœ€å°è¯´è¯äººæ•°ï¼Œnull ä¸ºè‡ªåŠ¨
  max_speakers: null          # æœ€å¤§è¯´è¯äººæ•°ï¼Œnull ä¸ºè‡ªåŠ¨

# LLM é…ç½®ï¼ˆä½¿ç”¨æœ¬åœ° vLLMï¼‰
llm:
  enabled: true               # æ˜¯å¦å¯ç”¨ LLM ä¼˜åŒ–
  provider: "vllm"            # vllm, openai, ollama, azure, anthropic
  model: "Qwen/Qwen3-32B"     # æ¨¡å‹åç§°
  base_url: "http://localhost:8000/v1"  # vLLM æœåŠ¡åœ°å€
  api_key: ""                 # vLLM æœ¬åœ°éƒ¨ç½²æ— éœ€ API key
  
  # åŠŸèƒ½å¼€å…³
  features:
    semantic_segmentation: true   # è¯­ä¹‰åˆ†æ®µ
    error_correction: true        # ASR é”™è¯¯ä¿®å¤
    expression_optimization: true # è¡¨è¾¾ä¼˜åŒ–

# ç¿»è¯‘é…ç½®ï¼ˆå¯é€‰ï¼‰
translation:
  enabled: false
  target_language: "zh"           # ç›®æ ‡è¯­è¨€
  style: "natural"                # natural, formal, casual

# è¾“å‡ºé…ç½®
output:
  formats: ["json", "srt", "vtt"]
  include_word_timestamps: true
  include_confidence: true
```

### ç¯å¢ƒå˜é‡

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# Hugging Face Tokenï¼ˆå¿…éœ€ï¼Œç”¨äº pyannote è¯´è¯äººåˆ†ç¦»ï¼‰
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx
```

### å¯åŠ¨ vLLM æœåŠ¡

```bash
# å¯åŠ¨ vLLM æœåŠ¡ï¼ˆQwen3-32Bï¼‰
vllm serve Qwen/Qwen3-32B --port 8000 --tensor-parallel-size 1
```

### å¯åŠ¨æœåŠ¡

```bash
# å¼€å‘æ¨¡å¼
uv run python -m whisperlm.main

# ç”Ÿäº§æ¨¡å¼
uv run gunicorn whisperlm.main:app -w 1 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8001
```

## ğŸ“– API æ–‡æ¡£

### æ ¸å¿ƒæ¥å£ï¼šè½¬å½•

**POST** `/api/v1/transcribe`

```bash
curl -X POST "http://localhost:8002/api/v1/transcribe" \
  -F "file=@audio.mp3" \
  -F "language=en" \
  -F "diarization=true" \
  -F "llm_optimize=true"
```

**è¯·æ±‚å‚æ•°ï¼š**

| å‚æ•° | ç±»å‹ | å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| file | File | âœ… | - | éŸ³é¢‘/è§†é¢‘æ–‡ä»¶ |
| language | string | âŒ | auto | è¯­è¨€ä»£ç ï¼Œauto ä¸ºè‡ªåŠ¨æ£€æµ‹ |
| diarization | bool | âŒ | true | æ˜¯å¦å¯ç”¨è¯´è¯äººåˆ†ç¦» |
| llm_optimize | bool | âŒ | true | æ˜¯å¦å¯ç”¨ LLM ä¼˜åŒ– |
| output_format | string | âŒ | json | è¾“å‡ºæ ¼å¼ï¼šjson/srt/vtt |
| min_speakers | int | âŒ | null | æœ€å°è¯´è¯äººæ•° |
| max_speakers | int | âŒ | null | æœ€å¤§è¯´è¯äººæ•° |

**å“åº”ç¤ºä¾‹ï¼š**

```json
{
  "task_id": "abc123",
  "status": "completed",
  "language": "en",
  "duration": 125.4,
  "speakers": ["SPEAKER_00", "SPEAKER_01"],
  "segments": [
    {
      "id": 0,
      "start": 0.0,
      "end": 4.52,
      "text": "Welcome to today's discussion about artificial intelligence.",
      "speaker": "SPEAKER_00",
      "words": [
        {"word": "Welcome", "start": 0.0, "end": 0.42, "confidence": 0.98},
        {"word": "to", "start": 0.44, "end": 0.52, "confidence": 0.99},
        {"word": "today's", "start": 0.54, "end": 0.92, "confidence": 0.97},
        {"word": "discussion", "start": 0.94, "end": 1.48, "confidence": 0.96},
        {"word": "about", "start": 1.50, "end": 1.72, "confidence": 0.99},
        {"word": "artificial", "start": 1.74, "end": 2.28, "confidence": 0.95},
        {"word": "intelligence.", "start": 2.30, "end": 3.12, "confidence": 0.94}
      ],
      "confidence": 0.97
    },
    {
      "id": 1,
      "start": 4.80,
      "end": 8.25,
      "text": "Thank you for having me. It's a pleasure to be here.",
      "speaker": "SPEAKER_01",
      "words": [...],
      "confidence": 0.96
    }
  ]
}
```

### è½¬å½• + ç¿»è¯‘

**POST** `/api/v1/transcribe-translate`

```bash
curl -X POST "http://localhost:8001/api/v1/transcribe-translate" \
  -F "file=@video.mp4" \
  -F "source_language=en" \
  -F "target_language=zh" \
  -F "translation_style=natural"
```

**é¢å¤–å‚æ•°ï¼š**

| å‚æ•° | ç±»å‹ | å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| target_language | string | âœ… | - | ç›®æ ‡è¯­è¨€ä»£ç  |
| translation_style | string | âŒ | natural | ç¿»è¯‘é£æ ¼ï¼šnatural/formal/casual |

**å“åº”ç¤ºä¾‹ï¼š**

```json
{
  "task_id": "def456",
  "status": "completed",
  "source_language": "en",
  "target_language": "zh",
  "segments": [
    {
      "id": 0,
      "start": 0.0,
      "end": 4.52,
      "text": "Welcome to today's discussion about artificial intelligence.",
      "translated_text": "æ¬¢è¿æ¥åˆ°ä»Šå¤©å…³äºäººå·¥æ™ºèƒ½çš„è®¨è®ºã€‚",
      "speaker": "SPEAKER_00",
      "confidence": 0.97
    }
  ]
}
```

### å…¼å®¹æ—§ç‰ˆæ¥å£

ä¸ºäº†å…¼å®¹æ—§ç‰ˆ STT æœåŠ¡ï¼ŒWhisperLM æä¾›ä»¥ä¸‹å…¼å®¹æ¥å£ï¼š

**POST** `/transcribe/`

```bash
curl -X POST "http://localhost:8001/transcribe/" \
  -F "file=@audio.mp3"
```

**å“åº”æ ¼å¼ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰ï¼š**

```json
{
  "status": "success",
  "results": [
    {
      "start": 0.0,
      "end": 4.52,
      "text": "Welcome to today's discussion about artificial intelligence."
    }
  ]
}
```

> æ³¨æ„ï¼šå…¼å®¹æ¥å£å†…éƒ¨ä½¿ç”¨ WhisperX ä¸€ä½“åŒ–å¤„ç†ï¼Œè¿”å›ç»“æœå·²åŒ…å«è¯´è¯äººä¿¡æ¯çš„ç²¾ç¡®å¯¹é½ï¼Œä½†å“åº”æ ¼å¼ä¿æŒæ—§ç‰ˆå…¼å®¹ã€‚

### å¥åº·æ£€æŸ¥

**GET** `/health`

```json
{
  "status": "healthy",
  "version": "1.0.0",
  "whisperx": {
    "model": "large-v3",
    "device": "cuda",
    "loaded": true
  },
  "diarization": {
    "model": "pyannote/speaker-diarization-3.1",
    "loaded": true
  },
  "llm": {
    "provider": "vllm",
    "model": "Qwen/Qwen3-32B",
    "connected": true
  },
  "gpu": {
    "available": true,
    "name": "NVIDIA RTX 4090",
    "memory_total": "24GB",
    "memory_used": "8GB"
  }
}
```


## ğŸ“ é¡¹ç›®ç»“æ„

```
whisperlm/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ whisperlm/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py                 # FastAPI åº”ç”¨å…¥å£
â”‚       â”œâ”€â”€ config.py               # é…ç½®ç®¡ç†
â”‚       â”œâ”€â”€ api/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ routes.py           # API è·¯ç”±
â”‚       â”‚   â”œâ”€â”€ legacy_routes.py    # å…¼å®¹æ—§ç‰ˆæ¥å£
â”‚       â”‚   â””â”€â”€ models.py           # Pydantic æ¨¡å‹
â”‚       â”œâ”€â”€ services/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ whisperx_service.py # WhisperX æ ¸å¿ƒæœåŠ¡
â”‚       â”‚   â”œâ”€â”€ llm_service.py      # LLM ä¼˜åŒ–æœåŠ¡
â”‚       â”‚   â””â”€â”€ task_service.py     # å¼‚æ­¥ä»»åŠ¡ç®¡ç†
â”‚       â”œâ”€â”€ processors/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ audio_processor.py  # éŸ³é¢‘é¢„å¤„ç†
â”‚       â”‚   â””â”€â”€ subtitle_processor.py # å­—å¹•æ ¼å¼è½¬æ¢
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ formats.py          # æ ¼å¼è½¬æ¢å·¥å…·
â”‚           â””â”€â”€ prompts.py          # LLM Prompts
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_transcribe.py
â”‚   â”œâ”€â”€ test_diarization.py
â”‚   â””â”€â”€ test_llm.py
â”œâ”€â”€ config.example.yaml
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

## âš™ï¸ é«˜çº§é…ç½®

### LLM Prompt è‡ªå®šä¹‰

ç¼–è¾‘ `src/whisperlm/utils/prompts.py`ï¼š

```python
# è¯­ä¹‰åˆ†æ®µä¼˜åŒ– Prompt
SEMANTIC_SEGMENTATION_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­—å¹•ç¼–è¾‘ã€‚è¯·å¯¹ä»¥ä¸‹è½¬å½•æ–‡æœ¬è¿›è¡Œè¯­ä¹‰åˆ†æ®µä¼˜åŒ–ã€‚

è§„åˆ™ï¼š
1. æ¯æ®µåº”è¯¥æ˜¯ä¸€ä¸ªå®Œæ•´çš„è¯­ä¹‰å•å…ƒï¼ˆä¸€ä¸ªå®Œæ•´çš„æƒ³æ³•/è§‚ç‚¹ï¼‰
2. ä¿æŒåŸæœ‰æ—¶é—´æˆ³çš„å‡†ç¡®æ€§
3. ä¿®å¤æ˜æ˜¾çš„ ASR é”™è¯¯ï¼ˆå¦‚é”™åˆ«å­—ã€æ¼å­—ï¼‰
4. ä¸è¦æ”¹å˜åŸæ„ï¼Œä¿æŒå£è¯­åŒ–è¡¨è¾¾

è¾“å…¥ï¼š
{transcription}

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºä¼˜åŒ–åçš„å­—å¹•ã€‚
"""

# ç¿»è¯‘ä¼˜åŒ– Prompt
TRANSLATION_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{target_language}ç¿»è¯‘ã€‚è¯·ç¿»è¯‘ä»¥ä¸‹å­—å¹•ã€‚

è¦æ±‚ï¼š
1. ä¿æŒå£è¯­åŒ–ã€è‡ªç„¶æµç•…
2. é€‚å½“è°ƒæ•´è¯­åºä»¥ç¬¦åˆ{target_language}è¡¨è¾¾ä¹ æƒ¯
3. ä¿ç•™ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§
4. æ§åˆ¶æ¯æ¡å­—å¹•é•¿åº¦ï¼Œé€‚åˆé˜…è¯»

åŸæ–‡ï¼š
{text}

ç¿»è¯‘ï¼š
"""
```

### æ€§èƒ½è°ƒä¼˜

```yaml
# æ¨èé…ç½®ï¼ˆ8GB+ GPUï¼‰- ç²¾åº¦æœ€é«˜
whisperx:
  model: "large-v3"
  compute_type: "float16"     # float16ï¼Œç²¾åº¦é«˜ï¼Œæ˜¾å­˜å ç”¨ ~8GB
  batch_size: 16

# é«˜æ€§èƒ½é…ç½®ï¼ˆ16GB+ GPUï¼‰
whisperx:
  model: "large-v3"
  compute_type: "float16"
  batch_size: 32              # æ›´å¤§æ‰¹æ¬¡ï¼Œé€Ÿåº¦æ›´å¿«

# ä½æ˜¾å­˜é…ç½®ï¼ˆ8GB GPU æ˜¾å­˜ç´§å¼ æ—¶ï¼‰
whisperx:
  model: "large-v3"
  compute_type: "int8"        # int8 é‡åŒ–ï¼Œæ˜¾å­˜å ç”¨ ~5GB
  batch_size: 16
```

### ä½¿ç”¨æœ¬åœ° LLMï¼ˆvLLM + Qwen3-32Bï¼‰

```bash
# å¯åŠ¨ vLLM æœåŠ¡
vllm serve Qwen/Qwen3-32B --port 8000

# æˆ–ä½¿ç”¨æ›´å¤š GPU å¹¶è¡Œ
vllm serve Qwen/Qwen3-32B --port 8000 --tensor-parallel-size 2
```

```yaml
# config.yaml é…ç½®
llm:
  provider: "vllm"
  model: "Qwen/Qwen3-32B"
  base_url: "http://localhost:8001/v1"
  api_key: ""
```

## ğŸ”§ å¸¸è§é—®é¢˜

### 1. pyannote æƒé™ç”³è¯·

1. è®¿é—® https://huggingface.co/pyannote/speaker-diarization-3.1
2. ç‚¹å‡» "Access repository" ç”³è¯·æƒé™
3. è·å– Hugging Face Tokenï¼šhttps://huggingface.co/settings/tokens
4. è®¾ç½®ç¯å¢ƒå˜é‡ `HF_TOKEN`

### 2. CUDA å†…å­˜ä¸è¶³

```yaml
# é™ä½æ¨¡å‹å¤§å°å’Œæ‰¹æ¬¡
whisperx:
  model: "medium"      # ä½¿ç”¨å°æ¨¡å‹
  compute_type: "int8" # ä½¿ç”¨ int8 é‡åŒ–
  batch_size: 4        # å‡å°æ‰¹æ¬¡å¤§å°
```

### 3. è½¬å½•é€Ÿåº¦æ…¢

- ç¡®ä¿ä½¿ç”¨ GPUï¼ˆ`device: "cuda"`ï¼‰
- å¢å¤§ `batch_size`
- ä½¿ç”¨ `compute_type: "float16"` è€Œé `float32`

### 4. è¯´è¯äººåˆ†ç¦»ä¸å‡†ç¡®

```yaml
diarization:
  min_speakers: 2      # æ˜ç¡®æŒ‡å®šè¯´è¯äººæ•°é‡
  max_speakers: 2
```

### 5. LLM è¶…æ—¶

```yaml
llm:
  timeout: 120         # å¢åŠ è¶…æ—¶æ—¶é—´
  max_retries: 3       # å¢åŠ é‡è¯•æ¬¡æ•°
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

| é…ç½® | 10åˆ†é’ŸéŸ³é¢‘å¤„ç†æ—¶é—´ | GPU æ˜¾å­˜å ç”¨ |
|------|-------------------|--------------|
| large-v3 + float16 | ~45s | ~8GB |
| large-v3 + int8 | ~60s | ~5GB |
| medium + float16 | ~25s | ~4GB |
| small + float16 | ~15s | ~2GB |
| small + CPU | ~180s | - |

*æµ‹è¯•ç¯å¢ƒï¼šNVIDIA RTX 4090, Intel i9-13900K*

## ğŸ”— ç›¸å…³é¡¹ç›®

- [WhisperX](https://github.com/m-bain/whisperX) - æ ¸å¿ƒè½¬å½•å¼•æ“
- [faster-whisper](https://github.com/SYSTRAN/faster-whisper) - é«˜æ€§èƒ½ Whisper å®ç°
- [pyannote-audio](https://github.com/pyannote/pyannote-audio) - è¯´è¯äººåˆ†ç¦»

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ï¼š`git checkout -b feature/amazing-feature`
3. æäº¤æ›´æ”¹ï¼š`git commit -m 'Add amazing feature'`
4. æ¨é€åˆ†æ”¯ï¼š`git push origin feature/amazing-feature`
5. æäº¤ Pull Request

